[
  {
    "CHUNK": "KAARTHIK ANDAVAR\n+64 2102256250 • kaarthikandavar@gmail.com • LinkedIn • GitHub • Medium • Portfolio\n\nEducation\n\n• University of Canterbury\n\nM.S - Data Science\n• Amrita University\n\nB.TECH - Electronics and Communication Engineering\n\nChristchurch, NZ\nJuly 2019 - July 2020\nCoimbatore, India\nApril 2013 - April 2017\n\nSkills Summary\n• Programming Languages: Python, SQL, Typescript\n• Data Science & ML: Snowpark, Pandas, NumPy, Scikit-Learn, Matplotlib, Langchain, Hugging Face\n• Databases: MySQL, Snowflake, PostgreSQL, MongoDB, DynamoDB\n• Cloud & DevOps: AWS, Git, GitHub, Pulumi, Terraform, Docker, Cloudflare\n• Full-Stack Development: HTML, Tailwind CSS, JavaScript, FastAPI, React, NextJS, HonoJS\n• Tools & Platforms: DBT, Airflow, AWS QuickSight, Streamlit, HEX, AWS Personalize, AWS Sagemaker, PowerBI\nExperience\n\n• Data & Analytics Engineer | Laybuy\n\nJuly 2021 - June 2024\n\nAuckland\n– Collaborated on data warehouse migration from Redshift to Snowflake, handling high-volume data.\n\n– Led ETL migration of ∼100 BI dashboards from Chartio to AWS Quicksight.\n\n– Reduced machine learning operational costs by 99.4% through migration from Datarobot to AWS SageMaker.\n\n– Engineered Recommendation Engine for Laybuy App using AWS Personalize, Airflow, and Snowflake.\n\n• Analyst (Data Science) | Stats NZ\n\nMarch 2021 - June 2021\n\nChristchurch\n– Performed Sentiment Analysis on NZ news articles from Common Crawl (40TB of raw web data).\n\n– Extracted global news from GDELT, monitoring news across 100+ languages.",
    "RELATIVE_PATH": "resume/2025-01-14/nN1N2gY0/temp_data_engineer_resume_example.pdf"
  },
  {
    "CHUNK": "KAARTHIK ANDAVAR\n+64 2102256250 • kaarthikandavar@gmail.com • LinkedIn • GitHub • Medium • Portfolio\n\nEducation\n\n• University of Canterbury\n\nM.S - Data Science\n• Amrita University\n\nB.TECH - Electronics and Communication Engineering\n\nChristchurch, NZ\nJuly 2019 - July 2020\nCoimbatore, India\nApril 2013 - April 2017\n\nSkills Summary\n• Programming Languages: Python, SQL, Typescript\n• Data Science & ML: Snowpark, Pandas, NumPy, Scikit-Learn, Matplotlib, Langchain, Hugging Face\n• Databases: MySQL, Snowflake, PostgreSQL, MongoDB, DynamoDB\n• Cloud & DevOps: AWS, Git, GitHub, Pulumi, Terraform, Docker, Cloudflare\n• Full-Stack Development: HTML, Tailwind CSS, JavaScript, FastAPI, React, NextJS, HonoJS\n• Tools & Platforms: DBT, Airflow, AWS QuickSight, Streamlit, HEX, AWS Personalize, AWS Sagemaker, PowerBI\nExperience\n\n• Data & Analytics Engineer | Laybuy\n\nJuly 2021 - June 2024\n\nAuckland\n– Collaborated on data warehouse migration from Redshift to Snowflake, handling high-volume data.\n\n– Led ETL migration of ∼100 BI dashboards from Chartio to AWS Quicksight.\n\n– Reduced machine learning operational costs by 99.4% through migration from Datarobot to AWS SageMaker.\n\n– Engineered Recommendation Engine for Laybuy App using AWS Personalize, Airflow, and Snowflake.\n\n• Analyst (Data Science) | Stats NZ\n\nMarch 2021 - June 2021\n\nChristchurch\n– Performed Sentiment Analysis on NZ news articles from Common Crawl (40TB of raw web data).\n\n– Extracted global news from GDELT, monitoring news across 100+ languages.",
    "RELATIVE_PATH": "resume/2025-01-14/nN1N2gY0/temp_kaarthik_Andavar_Senior_Data_Engineer_Resume.pdf"
  },
  {
    "CHUNK": "• Analyst (Data Science) | Stats NZ\n\nMarch 2021 - June 2021\n\nChristchurch\n– Performed Sentiment Analysis on NZ news articles from Common Crawl (40TB of raw web data).\n\n– Extracted global news from GDELT, monitoring news across 100+ languages.\n\n– Developed indicators for Covid-19 recovery monitoring and Data Portal updates.\n\n• Data Scientist (Internship) | Freightways Information Service\n\nAuckland\n– Implemented data modeling to enhance business process efficiency and forecasting.\n\n– Analyzed customer churn patterns to identify high-risk accounts and prevent business loss.\n\n– Developed customer segmentation system based on behavioral data analysis.\n\nNov 2020 - March 2021\n\n• Network Engineer | Mercedes-Benz Research and Development India\n\nNov 2017 - Jul 2019\n\nBengaluru\n– Managed client network infrastructure using BMC Remedy (CISM), ensuring optimal performance and security.\n\n– Conducted advanced troubleshooting of hardware/network issues, analyzing logs, debugs, and configurations.\n\n– Implemented and maintained network security policies, keeping infrastructure up-to-date and secure.\n\n– Handled critical cases and escalations, ensuring adherence to SLAs and minimizing downtime.\n\nProjects",
    "RELATIVE_PATH": "resume/2025-01-14/nN1N2gY0/temp_data_engineer_resume_example.pdf"
  },
  {
    "CHUNK": "• Analyst (Data Science) | Stats NZ\n\nMarch 2021 - June 2021\n\nChristchurch\n– Performed Sentiment Analysis on NZ news articles from Common Crawl (40TB of raw web data).\n\n– Extracted global news from GDELT, monitoring news across 100+ languages.\n\n– Developed indicators for Covid-19 recovery monitoring and Data Portal updates.\n\n• Data Scientist (Internship) | Freightways Information Service\n\nAuckland\n– Implemented data modeling to enhance business process efficiency and forecasting.\n\n– Analyzed customer churn patterns to identify high-risk accounts and prevent business loss.\n\n– Developed customer segmentation system based on behavioral data analysis.\n\nNov 2020 - March 2021\n\n• Network Engineer | Mercedes-Benz Research and Development India\n\nNov 2017 - Jul 2019\n\nBengaluru\n– Managed client network infrastructure using BMC Remedy (CISM), ensuring optimal performance and security.\n\n– Conducted advanced troubleshooting of hardware/network issues, analyzing logs, debugs, and configurations.\n\n– Implemented and maintained network security policies, keeping infrastructure up-to-date and secure.\n\n– Handled critical cases and escalations, ensuring adherence to SLAs and minimizing downtime.\n\nProjects",
    "RELATIVE_PATH": "resume/2025-01-14/nN1N2gY0/temp_kaarthik_Andavar_Senior_Data_Engineer_Resume.pdf"
  },
  {
    "CHUNK": "– Implemented and maintained network security policies, keeping infrastructure up-to-date and secure.\n\n– Handled critical cases and escalations, ensuring adherence to SLAs and minimizing downtime.\n\nProjects\n\n• snowChat: [GitHub] Developed an AI-based auto text-to-SQL chatbot for Snowflake using Streamlit, LangChain, GPT-4,\nand LLAMA models. Implemented in Python, enabling natural language queries to generate SQL for Snowflake databases\n• snowBrain AGUI: [GitHub] Created an AI-driven Snowflake insights platform with a chatbot capable of generating SQL\n\nand graphs from natural language input. Utilized Next.js, Tailwind CSS, Snowflake Node.js SDK, React Server\nComponents, and TypeScript for a responsive and efficient user interface.\n\n• snowSend: Engineered a system for sending dynamic emails directly from Snowflake in real-time. Leveraged AWS Lambda\n\nthrough Snowflake external functions, implemented with Pulumi and TypeScript for infrastructure as code.\n\n• Tryoutfit: Developed an AI-based virtual try-on web application using Next.js and AWS Amplify. Integrated AWS\n\nDynamoDB and S3 for data storage, and implemented an AI model for virtual outfit visualization.\n\nAchievements\n\n• Snowflake Streamlit AI Hackathon Winner (2023)\n• Cloudflare AI Hackathon Winner (2024)\n• Creator at Streamlit",
    "RELATIVE_PATH": "resume/2025-01-14/nN1N2gY0/temp_data_engineer_resume_example.pdf"
  },
  {
    "CHUNK": "– Implemented and maintained network security policies, keeping infrastructure up-to-date and secure.\n\n– Handled critical cases and escalations, ensuring adherence to SLAs and minimizing downtime.\n\nProjects\n\n• snowChat: [GitHub] Developed an AI-based auto text-to-SQL chatbot for Snowflake using Streamlit, LangChain, GPT-4,\nand LLAMA models. Implemented in Python, enabling natural language queries to generate SQL for Snowflake databases\n• snowBrain AGUI: [GitHub] Created an AI-driven Snowflake insights platform with a chatbot capable of generating SQL\n\nand graphs from natural language input. Utilized Next.js, Tailwind CSS, Snowflake Node.js SDK, React Server\nComponents, and TypeScript for a responsive and efficient user interface.\n\n• snowSend: Engineered a system for sending dynamic emails directly from Snowflake in real-time. Leveraged AWS Lambda\n\nthrough Snowflake external functions, implemented with Pulumi and TypeScript for infrastructure as code.\n\n• Tryoutfit: Developed an AI-based virtual try-on web application using Next.js and AWS Amplify. Integrated AWS\n\nDynamoDB and S3 for data storage, and implemented an AI model for virtual outfit visualization.\n\nAchievements\n\n• Snowflake Streamlit AI Hackathon Winner (2023)\n• Cloudflare AI Hackathon Winner (2024)\n• Creator at Streamlit",
    "RELATIVE_PATH": "resume/2025-01-14/nN1N2gY0/temp_kaarthik_Andavar_Senior_Data_Engineer_Resume.pdf"
  },
  {
    "CHUNK": "ALAN SUSA\nData Engineer\nalansusa@email.com\n\nLinkedIn\n\n(123) 456-7890\n\nNew York, NY\n\nEDUCATION\nB.A.\nComputer Science\nUniversity of Pittsburgh\n\nSeptember 2010 - April 2014\n\nPittsburgh, PA\n\nSKILLS\nPython\nETLs\nSQL (Postgres, Redshift, MySQL)\nNoSQL (MongoDB)\nSpark, Kafka\nAirﬂow\nAWS (Athena, Lambda, S3)\n\nWORK EXPERIENCE\n\nData Engineer\nConsumer Reports\nMay 2018 - current\n\nNew York, NY\n\nLed the migration from Oracle to Redshift using Amazon Athena\nand S3, resulting in an annual cost savings of $678,000 and an\nincrease in performance of 14%\nDesigned and implemented a real-time data pipeline to process\nsemi-structured data by integrating 150 million raw records\nfrom 30+ data sources using Kafka and PySpark\nDesigned the data pipeline architecture for a new product that\nquickly scaled from 0 to 125,000 daily active users\nStudied and revamped data dictionaries to include a more\nrobust history for developing consistency across domain\n\nData Engineer\nGuardian Life Insurance Company\n\nAugust 2016 - May 2018\n\nNew York, NY",
    "RELATIVE_PATH": "resume/2025-01-14/nN1N2gY0/temp_data_engineer_resume_example.pdf"
  },
  {
    "CHUNK": "Data Engineer\nGuardian Life Insurance Company\n\nAugust 2016 - May 2018\n\nNew York, NY\n\nMaintained data pipeline up-time of 99.8% while ingesting\nstreaming and transactional data across 8 diﬀerent primary\ndata sources using Spark, Redshift, S3, and Python\nAutomated ETL processes across billions of rows of data, which\nreduced manual workload by 29% monthly\nIngested data from disparate data sources using a combination\nof SQL, Google Analytics API, and Salesforce API using Python to\ncreate data views to be used in BI tools like Tableau\nCommunicated with project managers and analysts about data\npipelines that drove eﬃciency KPIs up by 26%\n\nData Engineer Intern\nFederal Reserve Board of Governors\n\nAugust 2014 - August 2016\n\nWashington, DC\n\nBuilt basic ETL that ingested transactional and event data from\na web app with 12,000 daily active users that saved over\n$85,000 annually in external vendor costs\nWorked with client to understand business needs and translate\nthose business needs into actionable reports in Tableau, saving\n17 hours of manual work each week\nUsed Spark in Python to distribute data processing on large\nstreaming datasets, improving ingestion and speed by 67%\nSupported implementation and active monitoring of controls\nand programs for precision and eﬃcacy",
    "RELATIVE_PATH": "resume/2025-01-14/nN1N2gY0/temp_data_engineer_resume_example.pdf"
  }
]